{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daba20f2",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6eb73a",
   "metadata": {},
   "source": [
    "Suppose a situation where the training data is not given all at once but rather little by little everyday. Should we train the model every day with the small portion of new data that accumulates? One disadvantage of such plan is that the data for training keeps getting bigger as time passes. It might work well for the first few days, but after months, we may need to increase the number of servers for training. What about after years? This is not a sustainable method. Another method we can consider is to discard old data when we accept new data, so that the overall size of the training set remains constant. However, this poses another problem because when we discard data, it's possible that we might lose critical information necessary for the model to predict a specific feature accurately.\n",
    "\n",
    "Another approach could be to avoid discarding the already trained set and instead train the model a little more with new data. This way, we don't need to maintain all the previously trained data, and the model will never forget what it has learned before. This kind of training is known as *incremental learning* or *online learning*. One of the incremental training algorithms is **stochastic gradient descent**, which is also provided as a class in scikit-learn.\n",
    "\n",
    "The term \"gradient\" implies the slope of a hill, and \"descent\" means how to go downhill. Therefore, \"gradient descent\" means the process of going downhill along the slope. The fastest way is the steepest path. The goal of gradient descent is to reach the target point by following the steepest path. However, if we take too big of one step, we may not go down but end up going up instead. Hence, it is best to follow the steepest path but important to descend *gradually*. This descending process is how we train a gradient descent model.  \"Stochastic\" is a technical term that essentially means \"random\" or \"randomly.\" In the context of gradient descent, the algorithm utilizes the training set to determine the steepest path. \n",
    "\n",
    "During the process of stochastic gradient descent, it selects only one sample randomly from the training set to find the steepest path. Instead of using all the available data at once, the algorithm updates the model's parameters incrementally, using a single randomly chosen sample at each step, and descends along the steepest slope a little bit. Then, it chooses another sample randomly from the training set and descends along the steepest slope again. It repeats this process until it uses the entire sample. If it has used all the samples and still hasn't reached the bottom of the hills, the algorithm simply starts the process anew. It refills the entire sample into the training set and then randomly chooses a sample to descend along the slope again. It does this until it reaches a satisfying position. In stochastic gradient descent, one process of using the entire training set is called an **epoch**. Generally, stochastic gradient descent involves more than hundreds of epochs.\n",
    "\n",
    "This randomness in sample selection allows the algorithm to approximate the steepest path with less computational overhead and is particularly useful when working with large datasets. By iteratively updating the model's parameters using different randomly selected samples, stochastic gradient descent can gradually optimize the model's performance and move towards the target point in a more efficient manner.\n",
    "\n",
    "So, since stochastic gradient descent chooses samples randomly, it must descend slowly; otherwise, it may end up on an irreversible path. However, stochastic gradient descent works quite well. But if we still have concerns, we can choose not just one sample per iteration to go down the slope but instead select a few samples randomly. Performing gradient descent this way, using a few samples, is called **minibatch gradient descent**. This approach is also widely used in real-world problems. On the other extreme, we can use the entire sample to make one descent, which is called **batch gradient descent**. Using the entire sample may be the most stable approach, but it requires much more computing resources. In some cases, having too much data to process at once may overwhelm the model, making it unable to handle the entire dataset in one go.\n",
    "\n",
    "In summary, stochastic gradient descent is an algorithm that gradually moves towards the optimal point below, even when we don't have the entire dataset and receive only a few data updates every day. This allows us to continue training without starting from the top every time.\n",
    "\n",
    "But what is the point from which we start descending? In other words, what is the metaphorical mountain that we want to descend by finding the fastest path? The answer is the loss function. The \"mountain\" we want to go down is represented by the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77d51d",
   "metadata": {},
   "source": [
    "> Neural network algorithms typically require a significant amount of data, making it impractical to perform calculations using the entire dataset at once. As a result, stochastic gradient descent or mini-batch gradient descent is commonly employed in neural network algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281c742",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a93ab",
   "metadata": {},
   "source": [
    "The loss function serves as a measure of how poorly a machine learning algorithm is performing. In this context, a smaller value of the loss function indicates better performance. However, determining the exact value of the local minimum is often unknown. As a result, we make ever effort to find it, and must accept that we have reached a local minimum when certain conditions are met. Stochastic gradient descent is a method that allows us to make incremental movements in order to find such a local minimum.\n",
    "\n",
    "The term \"Cost function\" is synonymous with \"loss function.\" Strictly speaking, a loss function defines the loss for an individual sample, while a cost function implies the summation or aggregation of the loss function values for the entire sample. However we often use both terms interchangeably and do not strictly differentiate between them.\n",
    "\n",
    "For a classification problem, the concept of loss is very clear; it represents the discrepany between the predictions and the actual answers. Let's consider a binary classification task distinguish between `Bream` and `Smelt`. We will label `Smelt` as the positive class (1) and `Bream` as the negative class (0), and assume the following predictions:\n",
    "\n",
    "|Predict | Answer (Target) |\n",
    "|--------|----------------|\n",
    "|1 | 1|\n",
    "|0 | 1|\n",
    "|0 | 0|\n",
    "|1 | 0|\n",
    "\n",
    "Among the four predictions, two are correct. Thus the accuracy is 2/4 = 0.5. Can accuracy be used as a loss function? Since a higher return of the loss function should indicate a poorly predicted outcome, we could multiply the accuracy by -1 to transform it so that the lowest return becomes -1.0(the best case) and the highest value becomes -0.0(the worst case).\n",
    "\n",
    "However, this accuracy metric has a fatal flaw. When dealing with only four samples, the possible accuracy values are limited to five: 0.0, 0.25, 0.5, 0.75 and 1. This lack of continuity makes it unsuitable for use as a loss function. In gradient descent, which operates in small increments, we require a loss function with continuous slopes. In mathe matical terms, a loss function should be differentiable.\n",
    "\n",
    "To archieve a continuous loss function, the logistic loss function is commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf88cce",
   "metadata": {},
   "source": [
    "## Logistic loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c99884",
   "metadata": {},
   "source": [
    "In binary classification, predictions can take the discrete values of 0 or 1, whereas the probabilities associated with these predictions lie in the continuous range between 0 and 1. Let's consider the probability predictions for the previous four samples:\n",
    "\n",
    "| Predict (Probability) | Answer (Target) |\n",
    "|----------------------|----------------|\n",
    "| 0.9                  | 1.0            |\n",
    "| 0.3                  | 1.0            |\n",
    "| 0.2                  | 0.0            |\n",
    "| 0.8                  | 0.0            |\n",
    "\n",
    "To transform these probabilities into a loss value, we can start by multiplying each prediction with its corresponding target and then negate the result:\n",
    "\n",
    "- For the first sample, with a prediction of 0.9 and a target of 1.0:\n",
    "  $$0.9 \\times 1.0 \\longrightarrow -0.9.$$\n",
    "\n",
    "- For the second sample, with a prediction of 0.3 and a target of 1.0:\n",
    "  $$0.3 \\times 1.0 \\longrightarrow -0.3.$$\n",
    "\n",
    "- However, for the third and fourth samples, since the targets are 0.0, we need to consider the complementary value (1 - prediction) as if they were predictions for the positive class:\n",
    "  $$0.2 \\times 1.0 \\longrightarrow -0.8.$$\n",
    "  $$0.8 \\times 1.0 \\longrightarrow -0.2.$$\n",
    "\n",
    "By following this procedure, we obtain the loss values as shown in the table below:\n",
    "\n",
    "| Predict (Probability) | Answer (Target) | Loss  |\n",
    "|----------------------|----------------|-------|\n",
    "| 0.9                  | 1.0            | -0.9  |\n",
    "| 0.3                  | 1.0            | -0.3  |\n",
    "| 0.2                  | 0.0            | -0.8  |\n",
    "| 0.8                  | 0.0            | -0.2  |\n",
    "\n",
    "Thus, the first and second samples have lower loss values. By applying this transformation to the probability values, we obtain a continuous loss function. Furthermore, it is beneficial to use a logarithm before multiplying by -1, as the prediction values range between 0.0 and 1.0. Taking the logarithm magnifies the loss for poorly predicted samples:\n",
    "\n",
    "- If the target value is 1, we apply $-\\log(\\text{predict})$.\n",
    "- If the target value is 0, we apply $-\\log(1 - \\text{predict})$.\n",
    "\n",
    "This loss function is commonly known as the **logistic loss function** or **binary cross-entropy loss function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c19fba3",
   "metadata": {},
   "source": [
    "## For multi-class classification problems and regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef187904",
   "metadata": {},
   "source": [
    "For multi-class classification problems, a similar loss function called the **cross-entropy loss function** is utilized.\n",
    "\n",
    "In practice, we rarely need to create our own loss function from scratch, since suitable loss functions have already been developed for various problems. For binary classification, the logistic loss function is commonly employed, while multiclass classification often uses the cross-entropy loss function.\n",
    "\n",
    "In regression problems, we have different options for loss functions. One approach is to use the \"mean absolute error (MAE),\" which involves computing the absolute difference between the predicted and target values for each sample and then taking the average. Alternatively, the \"mean squared error (MSE)\" can be employed, which involves squaring the difference between the prediction and target values for each sample, followed by averaging across all samples. Smaller values of these loss functions indicate better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b6bd9",
   "metadata": {},
   "source": [
    "## `SGDClassifier` for stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79dc2e",
   "metadata": {},
   "source": [
    "Let's make a classcification model by using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6051e410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Species  Weight  Length  Diagonal   Height   Width\n",
      "0   Bream   242.0    25.4      30.0  11.5200  4.0200\n",
      "1   Bream   290.0    26.3      31.2  12.4800  4.3056\n",
      "2   Bream   340.0    26.5      31.1  12.3778  4.6961\n",
      "3   Bream   363.0    29.0      33.5  12.7300  4.4555\n",
      "4   Bream   430.0    29.0      34.0  12.4440  5.1340\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fish = pd.read_csv('fish.csv')\n",
    "print(fish.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60a9b6",
   "metadata": {},
   "source": [
    "`Species` will be the target and other features will be the input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ef70a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "fish_target = fish['Species'].to_numpy()\n",
    "fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(\n",
    "    fish_input, fish_target, random_state = 42)\n",
    "\n",
    "ss = StandardScaler() # normalizer\n",
    "ss.fit(train_input) # train the normalizer\n",
    "\n",
    "# Normalizing\n",
    "train_scaled = ss.transform(train_input)\n",
    "test_scaled = ss.transform(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d29161",
   "metadata": {},
   "source": [
    "In scikit-learn, the `SGDClassifier` class offers stochastic gradient descent for classification tasks. When creating an instance of `SGDClassifier`, we are required to specify two parameters. `loss` determines the type of loss function to be used. In this case, we set `loss = 'log_loss'` to use the logistic loss function. `max_iter`, defines the number of epochs to be performed during training. Here, we assign the value `10`, meaning that the entire training set will be iterated over 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60743741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.773109243697479 (training data)\n",
      "score : 0.775 (test data)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)\n",
    "sc.fit(train_scaled, train_target)\n",
    "\n",
    "# Scores\n",
    "score_by_training = sc.score(train_scaled, train_target)\n",
    "score = sc.score(test_scaled, test_target)\n",
    "print(f\"score : {score_by_training} (training data)\")\n",
    "print(f\"score : {score} (test data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc27ce2",
   "metadata": {},
   "source": [
    "The number of iterations 10 might be insufficient. To avoid creating another another instance of the `SGDClassifier` class and to continue training further, we can utilize the `partial_fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35b6f1c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.8151260504201681 (training data)\n",
      "score : 0.85 (test data)\n"
     ]
    }
   ],
   "source": [
    "sc.partial_fit(train_scaled, train_target)\n",
    "\n",
    "# Scores\n",
    "score_by_training = sc.score(train_scaled, train_target)\n",
    "score = sc.score(test_scaled, test_target)\n",
    "print(f\"score : {score_by_training} (training data)\")\n",
    "print(f\"score : {score} (test data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d413b",
   "metadata": {},
   "source": [
    "Even though we have a lower score, executing one more epoch resulted in an imporved in accuracy. This indicates that we should continue training the model for more epochs. However, to determine the appropriate number of repetitions, we need a standard or a criterion to guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
