{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c09c046",
   "metadata": {},
   "source": [
    "# The Modern History of AI\n",
    "\n",
    "**1943:** Warren McCulloch and Walter Pitts speculated that a biological brain, composed of neurons, which are basic logical units, could be modeled as a combination of active and inactive binary circuits. They demonstrated a Turing machine can be implemented using a finite network of formal neurons.\n",
    "\n",
    "**1950:** Alan Turing defined the elements that could serve as standards for thinking machines in his paper \"Computing Machinery and Intelligence.\" Tests based on these standards are conventionally referred to as \"Turing Tests.\" However, Turing himself did not provide specific criteria for artificial intelligence in his paper, such as a specific value of passing threshold.\n",
    "\n",
    "**1956:** The term AI first appeared at the Dartmouth AI Conference.\n",
    "\n",
    "**1957:** Frank Rosenblatt of Cornell Aeronautical Laboratory announced the Perceptron. The Perceptron is a concept that learns by comparing input values one at a time and adjusting weights until a desired output value is achieved.\n",
    "\n",
    "**1959:** David Hubel and Torsten Viesel conducted a study on the function of neurons in the visual cortex using cats. They discovered that each neuron accepts a specific part of the visual field, which inspired the development of artificial neural networks for image recognition.\n",
    "\n",
    "**1969-:** Marvin Minsky and Seymour Papert of MIT proved that perceptrons cannot classify data that cannot be linearly separated, a problem known as the XOR problem. The period following this discovery is referred to as AI's first winter.\n",
    "\n",
    "**1980:** Kunihiko Fukushima of the NHK Institute of Broadcasting Technology developed a neural network model called Neocognitron based on the structure of the visual cortex identified by Hubel and Viesel.\n",
    "\n",
    "**1986-:** Geoffrey Hinton et al. solved the XOR problem with a multilayer perceptron and the error backpropagation algorithm. The error backpropagation algorithm adjusts the weights between nodes in the hidden layer while propagating errors generated in the output layer back to the input layer.\n",
    "\n",
    "**1989:** Yann Lecun of AT&T Labs achieved success in recognizing handwritten digits with a model called LeNet-5, which was the first convolutional neural network.\n",
    "\n",
    "**1998:** Yann Lecun et al. developed Neocognitron and introduced the Convolutional Neural Network (CNN), which is still widely used today.\n",
    "\n",
    "**1990s:** Despite the success of CNN, as the number of layers increased, the problem of vanishing gradient emerged. Vanishing gradient refers to the phenomenon where backward-propagating errors gradually disappear through several layers, making it challenging to adjust the weights of the input layer. This led to the second winter of AI. \n",
    "\n",
    "Later, the vanishing gradient problem was mitigated by using the Rectified Linear Unit (ReLU) function as the activation function for neurons. The ReLU function returns 0 if the input is negative and itself if it is positive. In contrast, the sigmoid function, which was predominantly used in the past, reaches a limit where it remains constant after a certain level of input. On the other hand, the ReLU function overcomes this limitation by returning the input itself even for very large values. Additionally, the learning rate and overfitting problems were addressed by incorporating a regularization process into the learning algorithm.\n",
    "\n",
    "**2006:** Geoffrey Hinton et al., in a paper titled \"A fast learning algorithm for deep belief nets,\" used a limited Boltzmann machine to pre-process each layer of an artificial neural network, thereby overcoming the limitations of the error backpropagation algorithm.\n",
    "\n",
    "**2007:** David Cournapeau created an open-source machine learning library called scikit-learn as part of a Google Summer of Code project.\n",
    "\n",
    "**2007:** Yoshua Bengio of the University of Montreal released Theano, the first deep learning library.\n",
    "\n",
    "**2012:** Geoffrey Hinton et al. won the ILSVRC image classification competition with a CNN-based neural network model called AlexNet, achieving an impressive error rate of 15%.\n",
    "\n",
    "**2015:** Google released an open-source deep learning library called TensorFlow.\n",
    "\n",
    "**2016:** Google DeepMind's AlphaGo defeated Lee Sedol.\n",
    "\n",
    "**2018:** Facebook released an open-source deep learning library called PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111a482",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Machine learning is an algorithm that learns rules automatically from data without the need for manually programming each rule. A notable open-source machine learning library for Python is scikit-learn, which was introduced by David Cournapeau as part of his Google Summer of Code project in 2007. Cournapeau, who was completing his doctorate at Kyoto University at the time, developed scikit-learn to provide a comprehensive toolset for machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229cfe91",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Deep learning refers to a collection of machine learning algorithms that are based on artificial neural networks.\n",
    "\n",
    "In 1989, Yann Lecun achieved a breakthrough in recognizing handwritten digits with a model called LeNet-5, which is considered the first convolutional neural network. Another significant milestone was reached in 2012 when Geoffrey Hinton's team won the ImageNet image classification competition using a convolutional neural network model named AlexNet. Since then, convolutional neural networks have become widely utilized for image classification tasks.\n",
    "\n",
    "In November 2015, Google introduced TensorFlow, an open-source deep learning library. TensorFlow has been instrumental in advancing the field of deep learning. In 2018, Facebook released PyTorch, another open-source deep learning library. These libraries are specifically designed to support artificial neural network algorithms, albeit with some differences in implementation details. For instance, TensorFlow adopts a define-and-run approach, where a computation graph is defined beforehand and data is then passed through it. On the other hand, PyTorch follows a define-by-run paradigm, where the next computation node is defined sequentially while passing data.\n",
    "\n",
    "In the meantime, Keras, introduced by Google developer Fran√ßois Chollet in March 2015, provides a user-friendly Python API for deep learning. Although Keras itself is not a deep learning library, it functions as an interface on top of TensorFlow, an actual deep learning library. Keras is suitable for configuring relatively simple neural networks or utilizing existing features. Notably, since the release of TensorFlow 2.0 in 2017, Google has integrated Keras as a core component, making TensorFlow primarily operate through Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
