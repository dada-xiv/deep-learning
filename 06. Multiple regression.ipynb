{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daba20f2",
   "metadata": {},
   "source": [
    "# Multiple regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ed989",
   "metadata": {},
   "source": [
    "**Multiple regression** refers to the use of linear regression with multiple features instead of just one. If there are two features, linear regression learns to model a surface. In higher dimensions with many features, linear regression could represent complex models.\n",
    "\n",
    "To handle a large number of features, we can utilize the pandas Dataframe. It offers a multidimensional array structure and a variety of features for data analysis. Additionally, a Dataframe can be eaily converted into a NumPy array. To read a CSV file that contains a datset, we can utilize the read_csv() method. Once the dataset is read, we can use the to_numpy() method to convert the data set into a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319c0527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   length   height   width\n",
      "0     8.4     2.11    1.41\n",
      "1    13.7     3.53    2.00\n",
      "2    15.0     3.82    2.43\n",
      "3    16.2     4.59    2.63\n",
      "4    17.4     4.59    2.94\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('perch_full.csv')\n",
    "\n",
    "# print fist 5 rows of Dataframe\n",
    "print(df.head(5))\n",
    "\n",
    "# input data : [length, height, weight]\n",
    "perch_full = df.to_numpy()\n",
    "\n",
    "# target data\n",
    "perch_weight = np.array(\n",
    "    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, \n",
    "     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, \n",
    "     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, \n",
    "     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, \n",
    "     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, \n",
    "     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, \n",
    "     1000.0, 1000.0])\n",
    "\n",
    "# make training and test sets\n",
    "train_input, test_input, train_target, test_target = train_test_split(\n",
    "    perch_full, perch_weight, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe2ab7",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "From given features $a$ and $b$, we can create a new feature denoted as $a \\times b$. This process is known as **feature engineering**.\n",
    "\n",
    "Sciket-learn offers various classed under the **sklearn.preprocessing** modlue for creating and preprocessing features. These classes such as PolynomialFeatures are called '**transformers**'. Similar to model classed, transformer classes provide like fit() and transform() methods. In order to perform transformation, a training must be done first. The transform() method actually applies the transformation to the date, while the fit() method discovers new combination of features. Note that transformers manipulate and transform input data to create new features or modify the existing ones, without involving target data.\n",
    "\n",
    "Let's try the **PolynomailFeatures** transformer and apply it to a sample with two features, specifically $2$ and $3$. The 'get_features_names_out()' method shows how the transformed features are generated from combinations of the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1669d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4. 6. 9.]]\n",
      "['1' 'x0' 'x1' 'x0^2' 'x0 x1' 'x1^2']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# transformer object\n",
    "poly = PolynomialFeatures()\n",
    "\n",
    "# train the transfomer\n",
    "poly.fit([[2, 3]])\n",
    "\n",
    "print(poly.transform([[2, 3]]))\n",
    "print(poly.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555dfb9d",
   "metadata": {},
   "source": [
    "Then we obtain more features. **PolynomialFeatures** creates interaction terms up to the second degree, that is, it adds powered term for each feature and product terms between features. For example, if we have the feature $2$ and $3$, the transformer will add $2^2 = 4$, $3^3=9$, and $2\\times 3 = 6$ as additional terms. Additionally, the intercept coefficient $1$ is included, wich is for the the zeroth term, $x_0^0 x_1^0= 1$. This is combinations with repititions, \n",
    "\n",
    "$${}_2\\mathrm{H}_0 + {}_2\\mathrm{H}_1 + {}_2\\mathrm{H}_2 = 6.$$\n",
    "\n",
    "However, in Scikit-learn's linear model, the intercept term is added automatically, so we don't need to creat it manually. If we set **include_bias=False** when using PolynomialFeatures, the transformer will not create an additional feature for the intercept term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fa8b98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 3. 4. 6. 9.]]\n",
      "['x0' 'x1' 'x0^2' 'x0 x1' 'x1^2']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# transformer object\n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "\n",
    "# train the transfomer\n",
    "poly.fit([[2, 3]])\n",
    "\n",
    "print(poly.transform([[2, 3]]))\n",
    "print(poly.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ce55e",
   "metadata": {},
   "source": [
    "Now we apply this procedure to our 'train_input'. We save the transformed data from 'train_input' into the array 'train_poly' and check its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "345d3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input :  (42, 3)\n",
      "Train polynomial (by transformer):  (42, 9)\n",
      "['x0' 'x1' 'x2' 'x0^2' 'x0 x1' 'x0 x2' 'x1^2' 'x1 x2' 'x2^2']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# transformer object\n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "\n",
    "# train the transfomer\n",
    "poly.fit(train_input) \n",
    "\n",
    "# after training the transformer, apply it to transform the input datasets\n",
    "train_poly = poly.transform(train_input)\n",
    "test_poly = poly.transform(test_input)\n",
    "\n",
    "print(\"Train input : \", train_input.shape)\n",
    "print(\"Train polynomial (by transformer): \", train_poly.shape)\n",
    "print(poly.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d22b0",
   "metadata": {},
   "source": [
    "Now we train the multiple regression model with 'train_poly' and 'train_target':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba70c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.9903183436982124 (training data)\n",
      "score : 0.9714559911594145 (test data)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# declear the model class\n",
    "lr = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "lr.fit(train_poly, train_target)\n",
    "\n",
    "# R^2 scores\n",
    "score_by_training = lr.score(train_poly, train_target)\n",
    "score = lr.score(test_poly, test_target)\n",
    "print(f\"score : {score_by_training} (training data)\")\n",
    "print(f\"score : {score} (test data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6cdea",
   "metadata": {},
   "source": [
    "We observe that there is no longer an underfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb048f0",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "We can incorporate additional features by including higher-order terms such as 3rd powers and 4th powers. The maximum degree of polynomial terms can be defined using the **degree** parameter of the PolynomialFeature class. For instance, setting 'degree=5' implies that the transfomer will generate polynomial terms up to the 5th power. The default value is 'degree=2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16866896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input :  (42, 3)\n",
      "Train polynomial (by transformer):  (42, 55)\n",
      "score : 0.9999999999996176 (training data)\n",
      "score : -144.40585108215134 (test data)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# transformer as a 5th order polynomial \n",
    "poly = PolynomialFeatures(degree=5, include_bias=False)\n",
    "\n",
    "# train the transfomer\n",
    "poly.fit(train_input)\n",
    "#print(poly.get_feature_names_out())\n",
    "\n",
    "# after training the transformer, apply it to transform the input datasets\n",
    "train_poly = poly.transform(train_input)\n",
    "test_poly = poly.transform(test_input)\n",
    "\n",
    "print(\"Train input : \", train_input.shape)\n",
    "print(\"Train polynomial (by transformer): \", train_poly.shape)\n",
    "\n",
    "# declear the model class\n",
    "lr = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "lr.fit(train_poly, train_target)\n",
    "\n",
    "# R^2 scores\n",
    "score_by_training = lr.score(train_poly, train_target)\n",
    "score = lr.score(test_poly, test_target)\n",
    "print(f\"score : {score_by_training} (training data)\")\n",
    "print(f\"score : {score} (test data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810c4a6",
   "metadata": {},
   "source": [
    "Here the number of generated features is 55, since\n",
    "\n",
    "$${}_3\\mathrm{H}_1 + {}_3\\mathrm{H}_2 + {}_3\\mathrm{H}_3 + {}_3\\mathrm{H}_4 + {}_3\\mathrm{H}_5 = 55.$$\n",
    "\n",
    "Note that this number is greater than the number of data points in the dataset. As a result, the trained model acheives an almost perfect score on the training data, but the test score is significally lower, indicating a failure to generalize well.\n",
    "\n",
    "Increasing the number of features can make the linear model more powerful. However, such model tends to overfit the training set and fails to fit the test sets accurately. To address this issue, it appears that we need to reduce the number of features again, in order to find a better balance between model complexity and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
