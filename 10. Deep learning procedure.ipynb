{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00b29b8",
   "metadata": {},
   "source": [
    "# Deep learning procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7b45",
   "metadata": {},
   "source": [
    "## 유닛의 출력\n",
    "\n",
    "앞먹임 신경망(feed back neural network)은 층 모양으로 늘어선 유닛이 인접한 층(layer)들과만 결합하는 구조를 가지며, 정보가 입력 측에서 출력 측으로 한 방향으로만 흐른다. 다층 퍼셉트론(multi-layer perceptron)이라 부르기도 한다. 주의할 점은 다층 퍼셉트론은 단층 퍼셉트론을 다층화한 것이 아니라는 것이다. 원래의 퍼셉트론은 입출력 함수가 스텝형 함수로 제한되어 있지만, 앞먹임 신경망 혹은 다층 퍼셉트론에서는 그러한 제한이 없는 등의 근본적인 차이가 있다.\n",
    "\n",
    "퍼셉트론을 통과하는 유닛 한 개의 입력은 \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u &= w_1x_1 + w_2 x_2 +w_3x_3 +w_4x_4 + b \\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "w_1 & w_2 & w_3 & w_4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \n",
    "\\end{bmatrix} + b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "과 같다. 즉 유닛이 받는 총 입력은 각 입력마다 서로 다른 가중치(weight) $w_1$, $w_2$, $w_3$, $w_4$를 곱한 값을 모두 곱하고, 여기에 편향(bias)치를 더한 것이다. 이 유닛의 출력 $z$는 총 입력 $u$에 대한 활성화 함수(activation function)라고 불리는 함수 $f$의 함숫값\n",
    "\n",
    "$$z = f(u)$$\n",
    "\n",
    "이다.\n",
    "\n",
    "만약 유닛이 하나의 층(layer)에서 여러 개의 유닛 $u_1$, $u_2$, $u_3$으로 (이전 층으로부터 온 출력) $x_1$, $x_2$, $x_3$, $x_4$를 입력으로 받는다면 유닛 간의 결합은 모두 $3 \\times 4 = 12$개이며, 그 하나하나의 결합마다 각각 다른 가중치 $w_{ji}$가 주어진다. 여기서 세 개의 유닛이 받는 입력은 \n",
    "\n",
    "$$\\begin{align*}\n",
    "u_1 &= w_{11}x_1 + w_{12} x_2 +w_{13}x_3 +w_{14}x_4 + b_1 \\,,\\\\\n",
    "u_2 &= w_{21}x_1 + w_{22} x_2 +w_{23}x_3 +w_{24}x_4 + b_2 \\,,\\\\\n",
    "u_3 &= w_{31}x_1 + w_{32} x_2 +w_{33}x_3 +w_{34}x_4 + b_3 \n",
    "\\end{align*}$$\n",
    "\n",
    "과 같다. 즉, 가중치 $w_{ji}$는 $l$번째 층의 $i$번째 유닛의 출력이 $l+1$번째 층의 $j$번째 유닛의 입력으로 전달될 때 곱해지는 값이다. 여기에 다시 활성화 함수를 적용하면 출력은 \n",
    "\n",
    "$$\\begin{align*}\n",
    "z_1 &= f(u_1)\\,,\\\\\n",
    "z_2 &= f(u_2)\\,,\\\\\n",
    "z_3 &= f(u_3)\n",
    "\\end{align*}$$\n",
    "\n",
    "이 된다. 이는 행렬 표기법을 사용하면 \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{u} &= \\mathbf{W}\\mathbf{x} +\\mathbf{b}\\ \\,, \\\\\n",
    "\\mathbf{z} &= \\mathbf{f}(\\mathbf{u})\n",
    "\\end{align*}$$\n",
    "\n",
    "로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924d4e4",
   "metadata": {},
   "source": [
    "## 활성화 함수\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823913d",
   "metadata": {},
   "source": [
    "## 다층 신경망\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{u}^{(2)} &= \\mathbf{W}^{(2)}\\mathbf{x} +\\mathbf{b}^{(2)}\\ \\,, \\\\\n",
    "\\mathbf{z}^{(2)} &= \\mathbf{f}(\\mathbf{u} ^{(2)})\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{u}^{(3)} &= \\mathbf{W}^{(3)}\\mathbf{z}^{(2)} +\\mathbf{b}^{(3)}\\ \\,, \\\\\n",
    "\\mathbf{z}^{(3)} &= \\mathbf{f}(\\mathbf{u}^{(3)})\n",
    "\\end{align*}$$\n",
    "\n",
    "다층 신경망의 각각의 층 $l = 1\\,, 2\\,, \\ldots L$을 통과한 신경망의 최종 출력은 간단히 \n",
    "\n",
    "$$ \\mathbf{y} \\equiv \\mathbf{z}^{L} $$\n",
    "\n",
    "로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abfa798",
   "metadata": {},
   "source": [
    "## 출력층의 설계와 오차함수\n",
    "\n",
    "하나의 입력 $\\mathbf{x}$에 대하여 바람직한 출력을 $\\mathbf{d}$라 할 때, 이러한 입출력의 쌍\n",
    "\n",
    "$$\n",
    "\\mathcal{D} =\n",
    "\\{\n",
    "(\\mathbf{x}_1, \\mathbf{d}_1) \\,, \\ldots \\,, \n",
    "(\\mathbf{x}_N, \\mathbf{d}_N)\n",
    "\\}\n",
    "$$\n",
    "\n",
    "에 대하여 $(\\mathbf{x}_n, \\mathbf{d}_n)$ 하나하나를 훈련 샘플(training sample)이라 부르고, 이들의 집합 $\\mathcal{D}$를 훈련 데이터라 하자.\n",
    "\n",
    "신경망에서의 학습이란 모든 입출력 쌍 $(\\mathbf{x}_n, \\mathbf{d}_n)$($n=1,\\cdots,N$)에 대하여 입력 $\\mathbf{x}_n$이 주어진 신경망의 출력 \n",
    "$\\mathbf{y}(\\mathbf{x}_n; \\mathbf{w})$이 최대한 올바른 출력 $\\mathbf{d}_n$에 가까워지도록 신경망의 파라미터(가중치와 편향) $\\mathbf{w}$를 조정하는 과정이다.\n",
    "\n",
    "이때 신경망이 나타내는 함수 $\\mathbf{y}(\\mathbf{x}_n; \\mathbf{w})$와 훈련 데이터 $\\mathbf{d}_n$와의 가까운 척도를 오차 함수(error function) 혹은 비용 함수라고 부른다. 즉, 신경망의 학습은 오차 함수 $E(\\mathbf{w})$를 신경망의 파라미터(가중치와 편향) $\\mathbf{w}$에 대하여 최소화하는 과정이며, 학습의 목표는 최솟값을 주는 \n",
    "$\\mathbf{w} = \\text{argmin}_{\\mathbf{w}} E(\\mathbf{w})$를 구하는 것이 된다.\n",
    "\n",
    "### 회귀(regression)\n",
    "\n",
    "회귀에서는 보통 제곱오차(squared error) $\\lVert \\mathbf{d} - \\mathbf{y}(\\mathbf{x}; \\mathbf{w}) \\rVert^2$를 사용하는 오차 함수\n",
    "\n",
    "$$\n",
    "E( \\mathbf{w} ) = \\frac{1}{2} \\sum_{n=1}^{N}\n",
    "\\lVert \\mathbf{d} _n - \\mathbf{y}(\\mathbf{x} _n; \\mathbf{w}) \\rVert^2\n",
    "$$\n",
    "\n",
    "를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e085d",
   "metadata": {},
   "source": [
    "## 경사 하강법\n",
    "\n",
    "일반적인 경우 $E(\\mathbf{w})$의 전역 극소점은 존재하지 않을 것이며, 국소(local) 극소점을 구하고 그 값이 충분히 작다면 목적이 되는 분류나 회귀 문제를 나름대로 잘 풀 수 있다.\n",
    "\n",
    "국소 극소점 하나는 어떤 초기 점을 출발점으로 하고 $\\mathbf{w}$를 되풀이하여 갱신하는 반복 계산을 통해 구할 수 있다. 이중 가장 간단한 방법이 경사 하강법(gradient descent method)이다. 경사 하강법에서 기울기(gradient)란 벡터 \n",
    "\n",
    "$$\n",
    "\\nabla E \\equiv \\frac{\\partial E}{\\partial \\mathbf{w}}\n",
    "= \\left[ \n",
    "\\frac{\\partial E}{\\partial w_1}\n",
    "\\cdots\n",
    "\\frac{\\partial E}{\\partial w_M}\n",
    "\\right]^T\n",
    "$$\n",
    "\n",
    "이다. 현재 위치를 기준으로 $\\nabla E$가 양수인 경우 $E$의 값이 증가하는 방향을 나타내고, 음수인 경우 $E$ 값이 감소하는 방향을 나타낸다. 우리는 오차 함수를 최소화하는 것이 목적이므로, 이를 위해서는 현재 위치에서 음의 기울기 방향으로 이동해야 한다. 따라서 경사 하강법은 현재의 $\\mathbf{w}$를 음의 기울기 방향($-\\nabla E$)으로 조금씩 움직이는 것을 여러 번 반복하는 과정이다.\n",
    "\n",
    "구체적으로는, 현재의 가중치를 $\\mathbf{w}^{(t)}$, 이동 후의 가중치를 $\\mathbf{w}^{(t+1)}$이라 하면, \n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} - \\mathbf{w}^{(t)} = \\epsilon \\nabla E\n",
    "$$\n",
    "\n",
    "가 되므로 \n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\epsilon \\nabla E\n",
    "$$\n",
    "\n",
    "과 같이 갱신한다. 여기서 $\\epsilon$은 $\\mathbf{w}$의 갱신량의 크기를 결정하는 값으로, 학습률(learning rate)라고 부른다. 초깃값 $\\mathbf{w}^{(1)}$을 적당히 결정한 뒤 위 식을 되풀이하는 과정을 통해 $\\mathbf{w}^{(2)}$, $\\mathbf{w}^{(3)}$, $\\cdots$을 얻을 수 있다.\n",
    "\n",
    "$\\epsilon$의 값이 너무 작다면 $\\mathbf{w}$가 한 번 갱신될 때 변화하는 양이 너무 적어지므로 반복을 위한 횟수가 많이 필요하고, 따라서 학습에 걸리는 시간이 늘어나게 된다. 학습률 $\\epsilon$을 잘 결정하는 것은 학습이 잘 진행되기 위한 핵심 요소 중 하나이다.\n",
    "\n",
    "문제 데이터의 크기가 큰 경우 목적함수에 대한 2차 미분 조차도 어려우므로 간신히 1차 미분, 즉 기울기 밖에 계산할 수 없는 경우가 있을 수 있는데, 딥러닝의 신경망은 이와 같은 경우에 해당하는 대표적인 예이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf1b40",
   "metadata": {},
   "source": [
    "## 역전파\n",
    "\n",
    "경사 하강법을 위해서는 오차 함수 $E(\\mathbf(w))$의 기울기 $\\nabla E = \\partial E / \\partial \\mathbf{w}$를 계산해야 한다. 그러나 이들 미분을 계산하는 것은 중간층, 특히 입력에서 가까운 쪽은 매개변수들일수록 까다로워진다. 이를테면 하나의 샘플 $\\mathbf{x}_n$에 대한 제곱 오차\n",
    "\n",
    "$$\n",
    "E_n = \\frac{1}{2} \n",
    "\\lVert \\mathbf{d} _n - \\mathbf{y}(\\mathbf{x} _n) \\rVert^2\n",
    "$$\n",
    "\n",
    "을 $l$번째 층의 가중치 $w_{ji}^{(l)}$으로 미분하면 다음과 같이 된다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}^{(l)}}\n",
    "= (\\mathbf{y}(\\mathbf{x} _n) - \\mathbf{d} _n)^T \n",
    "\\frac{\\partial \\mathbf{y}}{\\partial w_{ji}^{(l)}}\n",
    "$$\n",
    "\n",
    "그런 다음 우변의 미분 $\\partial \\mathbf{y}/\\partial w_{ji}^{(l)}$을 구해야 하지만, \n",
    "여기서\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{y}(\\mathbf{x}) \n",
    "& = \\mathbf{f}(\\mathbf{u}^{(L)}) \\\\ \n",
    "& = \\mathbf{b}(\\mathbf{W}^{(L)}\\mathbf{z}^{(L-1)}+\\mathbf{b}^{(L)}) \\\\ \n",
    "& = \\mathbf{b}(\\mathbf{W}^{(L)}\\mathbf{b}(\\mathbf{W}^{(L-1)}\\mathbf{z}^{(L-2)}+\\mathbf{b}^{(L-1)})+\\mathbf{b}^{(L)}) \\\\ \n",
    "& = \\mathbf{b}(\\mathbf{W}^{(L)}(\\mathbf{W}^{(L-1)}\\mathbf{b}(\\mathbf{W}^{(L-2)}\\mathbf{z}^{(L-3)}+\\mathbf{b}^{(L-2)})+\\mathbf{b}^{(L-1)})+\\mathbf{b}^{(L)}) \\\\ \n",
    "& \\cdots \\\\\n",
    "& = \\mathbf{b}(\\mathbf{W}^{(L)}(\\mathbf{W}^{(L-1)}\\mathbf{b}(\\cdots \\mathbf{b}(\\mathbf{W}^{(l)}\\mathbf{z}^{(l-1)}+\\mathbf{b}^{(l)}) \\cdots)+\\mathbf{b}^{(L-1)})+\\mathbf{b}^{(L)}) \n",
    "\\end{align*}$$\n",
    "\n",
    "즉, 함수 $\\mathbf{y}(\\mathbf{x} _n)$ 안에 $w_{ji}^{(l)}$가 활성화 함수가 몇 겹이나 적용되는 깊은 식 안에 있기 때문에 미분의 연쇄법칙(chain rule)을 여러 번 적용해야만 하는 어려움이 있다. 역전파(backpropagation)는 이러한 문제를 해결하는 방법이다.\n",
    "\n",
    "표현의 간소화를 위해 항상 +1을 출력하는 특수한 0번째 유닛을 각 층마다 추가하여 편향을 $b_j^{l} = w_{0j}^{(l)}$로 표기하자. 그러면 다음\n",
    "\n",
    "$$\n",
    "u_j^{(l)} = \\sum_{i=1}^n w_{ji}^{(l)}z_i^{(l-1)} + b_j\n",
    "= \\sum_{i=0}^n w_{ji}^{(l)}z_i^{(l-1)}\n",
    "$$\n",
    "\n",
    "과 같이 쓸 수 있다. 또한 그 뜻이 명시적일 경우에는 각각의 샘플을 가리키는 $n$을 생략하여 $\\mathbf{x}_n \\rightarrow \\mathbf{x}$이라고 나타내자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed498789",
   "metadata": {},
   "source": [
    "### 두 개의 층으로 구성된 신경망\n",
    "\n",
    "간단히 하기 위해 출력층의 활성화 함수를 항등 사상으로 놓자. 그러면 입력 $\\mathbf{x} = [ x_1 x_2 x_3 x_4 ]^T$는 출력층으로 다음과 같이 전파된다. 입력층의 출력을 $z_i(1) = x_i$이라 표기하면 중간층의 출력은 \n",
    "\n",
    "$$\n",
    "z_j^{(2)} = f(u_j^{(2)}) \n",
    "= f \\left(\n",
    "\\sum_{i} w_{ji}^{(2)} z_i^{(1)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "이 된다. 출력 함수는 항등사상이라고 가정했으므로 출력은 다음\n",
    "\n",
    "$$\n",
    "y_j(\\mathbf{x}) = z_j^{(3)} = u_j^{(3)}\n",
    "= \\sum_{i} w_{ji}^{(3)} z_i^{(2)}\n",
    "$$\n",
    "\n",
    "과 같다. 신경망의 오차함수로 제곱 오차를 선택했다면 \n",
    "\n",
    "$$\n",
    "E_n \n",
    "= \\frac{1}{2} \\lVert \\mathbf{y}(\\mathbf{x}) - \\mathbf{d} \\rVert^2\n",
    "= \\frac{1}{2} \\sum_{j} \\lVert y_j(\\mathbf{x}_n) - d_j \\rVert^2\n",
    "$$\n",
    "\n",
    "이제 가중치에 대한 미분 $\\partial E_n/\\partial w_{ji}^{(2)}$와 $\\partial E_n/\\partial w_{ji}^{(3)}$을 구해보자. 우선 출력층의 가중치에 대한 미분 $\\partial E_n/\\partial w_{ji}^{(3)}$은 쉽게 다음\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}^{(3)}}\n",
    "= (\\mathbf{y}(\\mathbf{x}) - \\mathbf{d})^T \n",
    "\\frac{\\partial \\mathbf{y}}{\\partial w_{ji}^{(3)}}\n",
    "$$\n",
    "\n",
    "과 같이 계산된다. 여기서 우변의 $\\partial \\mathbf{y}/\\partial w_{ji}^{(3)}$은 벡터이지만, 성분 $j$의 값만 $z_i^{(2)}$이고 나머지는 $0$인 다음과 같은 형태\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial w_{ji}^{(3)}} \n",
    "= [0 \\cdots 0 \\, z_i^{(2)} \\,  0 \\cdots 0]^T\n",
    "$$\n",
    "\n",
    "가 된다. 따라서 다음 \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}^{(3)}}\n",
    "= (y_j(\\mathbf{x}) - d_j) z_i^{(2)}\n",
    "$$\n",
    "\n",
    "을 얻는다. 이제 중간층의 기울기에 대한 미분 $\\partial E_n/\\partial w_{ji}^{(2)}$를 계산해보자. $ w_{ji}^{(2)}$는 $u_j^{(2)} = \\sum_{i} w_{ji}^{(2)} z_i^{(1)}$와 같이 $u_j^{(2)}$ 안에만 존재한다. 그러므로 $E_n$을 $u_j^{(2)}$로 미분하여 연쇄법칙을 적용하면, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}^{(2)}}\n",
    "= \\frac{\\partial E_n}{\\partial u_j^{(2)}}\n",
    "\\frac{\\partial u_j^{(2)}}{\\partial w_{ji}^{(2)}}\n",
    "$$\n",
    "\n",
    "와 같이 된다. 여기서 우변의 두 번째 항은 \n",
    "\n",
    "$$\n",
    "\\frac{\\partial u_j^{(2)}}{\\partial w_{ji}^{(2)}}\n",
    "= \n",
    "\\frac{\\partial}{\\partial w_{ji}^{(2)}}\n",
    "\\left(\\sum_{i} w_{ji}^{(2)} z_i^{(1)} \\right)\n",
    "=z_i^{(1)}\n",
    "$$\n",
    "\n",
    "이고, 우변의 첫 번째 항은\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial u_j^{(2)}}\n",
    "= \\sum_{k}\n",
    "\\frac{\\partial E_n}{\\partial u_k^{(3)}}\n",
    "\\frac{\\partial u_k^{(3)}}{\\partial u_j^{(2)}}\n",
    "$$\n",
    "\n",
    "이다. 이는 다시 \n",
    "\n",
    "$$\n",
    "E_n \n",
    "= \\frac{1}{2}\\sum_{k} (y_k(\\mathbf{x})-d_k)^2 \n",
    "= \\frac{1}{2}\\sum_{k} (u_k^{(3)}-d_k)^2 \n",
    "$$\n",
    "\n",
    "에서 \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial u_k^{(3)}}\n",
    "= u_k^{(3)}-d_k \n",
    "\\equiv \\delta_k^{(3)}\n",
    "$$\n",
    "\n",
    "이고, 또한 \n",
    "\n",
    "$$\n",
    "\\frac{\\partial u_k^{(3)}}{\\partial u_j^{(2)}}\n",
    "= \n",
    "\\frac{\\partial }{\\partial u_j^{(2)}}\n",
    "\\left(\\sum_{i} w_{ki}^{(3)} f(u_j^{(2)}) \\right)\n",
    "= f'(u_j^{(2)}) w_{kj}^{(3)}\n",
    "$$\n",
    "\n",
    "이므로 \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial u_j^{(2)}} \n",
    "= \\sum_{k}\n",
    "\\delta_k^{(3)}\n",
    "f'(u_j^{(2)}) w_{kj}^{(3)}\n",
    "\\equiv \\delta_j^{(2)}\n",
    "$$\n",
    "\n",
    "최종적으로 \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}^{(2)}}\n",
    "&= \\frac{\\partial E_n}{\\partial u_j^{(2)}} \n",
    "\\frac{\\partial u_j^{(2)}}{\\partial w_{ji}^{(2)}} \\\\\n",
    "&= \n",
    "\\left(\\sum_{k}\n",
    "\\frac{\\partial E_n}{\\partial u_k^{(3)}}\n",
    "\\frac{\\partial u_k^{(3)}}{\\partial u_j^{(2)}}\n",
    "\\right)\n",
    "z_i^{(1)} \\\\\n",
    "&= \\left(\\sum_{k}\n",
    "(u_k^{(3)}-d_k)\n",
    "\\left(f'(u_j^{(2)}) w_{kj}^{(3)}\\right)\n",
    "\\right)\n",
    "z_i^{(1)} \\\\\n",
    "&= f'(u_j^{(2)})\n",
    "\\left(\\sum_{k} w_{kj}^{(3)} (u_k^{(3)}-d_k)  \\right)\n",
    "z_i^{(1)} \n",
    "\\end{align*}$$\n",
    "\n",
    "또는 \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}^{(2)}}\n",
    "= \\left( f'(u_j^{(2)}) \\sum_{k}  \\delta_k^{(3)} w_{kj}^{(3)}  \\right) z_i^{(1)}\n",
    "= \\delta_j^{(2)} z_i^{(1)} \n",
    "$$\n",
    "\n",
    "을 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951c336",
   "metadata": {},
   "source": [
    "### 다층 신경망으로 일반화 \n",
    "\n",
    "여기서 임의의 층수를 갖는 신경망으로 확장하면 \n",
    "$\\delta_j^{(l)} = \\partial E_n / \\partial u_j^{(l)}$로 정의되는 값은 \n",
    "\n",
    "$$\n",
    "\\delta_j^{(l)} = f'(u_j^{(l)}) \\sum_{k} \\delta_k^{(l+1)}  w_{kj}^{(l+1)}\n",
    "$$\n",
    "\n",
    "의 관계를 가진다. 즉 더 멀리 있는 $l+1$층의 유닛에 대한 $\\delta_k^{(l+1)}$가 주어지면 이전 $l$층의 $\\delta_j$를 계산할 수 있게 된다. 이렇게 출력층까지 거슬러 올라가서 맨 처음 출력층의 유닛에 대한 $\\delta_j^{(N)}$의 값이 계산되어 있으면 위 식을 역방향 으로 반복하여 어떤 층에 대한 $\\delta_j^{(l)}$도 산출해낼 수 있게 된다. 이를 통해 오차 함수의 기울기 \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}^{(l)}} = \\delta_j^{(l)} z_i^{(1-1)} \n",
    "$$\n",
    "\n",
    "를 얻는다. 여기서 $\\delta_j^{(l)}$가 출력층에서 입력층으로 순전파와는 반대 방향으로 계산되므로 이러한 계산 과정을 역전파라고 부른다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
